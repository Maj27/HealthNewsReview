{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HNR new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWt29sTk-hZy"
      },
      "source": [
        "## Predicting quality of health news usig BERT (using Health news reviews dataset)\n",
        "# This code is a modification of https://github.com/kpe/bert-for-tf2/blob/master/examples/gpu_movie_reviews.ipynb which was orginally \n",
        "# modified from  https://github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
        "#using the Tensorflow 2.0 Keras implementation of BERT from kpe/bert-for-tf2 with the original google-research/bert weights."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT-7E2qs-7ay",
        "outputId": "c83709d2-f863-4832-d35c-cb95ce54e03b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Version:  2.3.0\n",
            "Eager mode:  True\n",
            "Hub version:  0.10.0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-7xkcQp-7iL"
      },
      "source": [
        "!pip install tqdm  >> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eweu-bEI-7k1"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQEA5I41-7nV",
        "outputId": "c326837b-d730-4bd3-8315-0f4198a79114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.3.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZOxi1aiDMPr"
      },
      "source": [
        "if tf.__version__.startswith(\"1.\"):\n",
        "  tf.enable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v0Rb4yNDMSE"
      },
      "source": [
        "!pip install bert-for-tf2 >> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BzMbfiLDMUf"
      },
      "source": [
        "import bert\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
        "from bert.tokenization.bert_tokenization import FullTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2AjG7tbUAGE",
        "outputId": "1762cc94-4012-4a0d-93e3-4a278d3666f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaxMVd7KDMWn"
      },
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Load all files from a directory in a DataFrame.\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in tqdm(os.listdir(directory), desc=os.path.basename(directory)):\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\w+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  #dataset = tf.keras.utils.get_file(\n",
        "  #    fname=\"aclImdb.tar.gz\", \n",
        "  #    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "  #    extract=True)\n",
        "  \n",
        "   #train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "  #                                     \"aclImdb\", \"train\"))\n",
        "  #test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
        "  #                                    \"aclImdb\", \"test\"))\n",
        "\n",
        "\n",
        "  #train_df = load_dataset(\"/content/drive/My Drive/Data/train\")\n",
        "  #test_df = load_dataset(\"/content/drive/My Drive/Data/test\")\n",
        "\n",
        "  train_df = load_dataset(\"/content/drive/My Drive/Data/Data10/train\")\n",
        "  test_df = load_dataset(\"/content/drive/My Drive/Data/Data10/test\")\n",
        "  \n",
        "  return train_df, test_df\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4PgMmFIDaKB"
      },
      "source": [
        "import bert\n",
        "from bert import BertModelLayer\n",
        "from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\n",
        "from bert import bert_tokenization\n",
        "\n",
        "\n",
        "class HealthReviewData:\n",
        "    DATA_COLUMN = \"sentence\"\n",
        "    LABEL_COLUMN = \"polarity\"\n",
        "\n",
        "    def __init__(self, tokenizer: FullTokenizer, sample_size=None, max_seq_len=8192):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.sample_size = sample_size\n",
        "        self.max_seq_len =  0\n",
        "        train, test = download_and_load_datasets()\n",
        "        \n",
        "        train, test = map(lambda df: df.reindex(df[HealthReviewData.DATA_COLUMN].str.len().sort_values().index), \n",
        "                          [train, test])\n",
        "                \n",
        "        if sample_size is not None:\n",
        "            assert sample_size % 128 == 0\n",
        "            train, test = train.head(sample_size), test.head(sample_size)\n",
        "            # train, test = map(lambda df: df.sample(sample_size), [train, test])\n",
        "        \n",
        "        ((self.train_x, self.train_y),\n",
        "         (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n",
        "\n",
        "        print(\"max seq_len\", self.max_seq_len)\n",
        "        self.max_seq_len = min(self.max_seq_len, max_seq_len)\n",
        "        ((self.train_x, self.train_x_token_types),\n",
        "         (self.test_x, self.test_x_token_types)) = map(self._pad, \n",
        "                                                       [self.train_x, self.test_x])\n",
        "\n",
        "    def _prepare(self, df):\n",
        "        x, y = [], []\n",
        "        with tqdm(total=df.shape[0], unit_scale=True) as pbar:\n",
        "            for ndx, row in df.iterrows():\n",
        "                text, label = row[HealthReviewData.DATA_COLUMN], row[HealthReviewData.LABEL_COLUMN]\n",
        "                tokens = self.tokenizer.tokenize(text)\n",
        "                tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "                token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "                self.max_seq_len = max(self.max_seq_len, len(token_ids))\n",
        "                x.append(token_ids)\n",
        "                y.append(int(label))\n",
        "                pbar.update()\n",
        "        return np.array(x), np.array(y)\n",
        "\n",
        "    def _pad(self, ids):\n",
        "        x, t = [], []\n",
        "        token_type_ids = [0] * self.max_seq_len\n",
        "        for input_ids in ids:\n",
        "            input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n",
        "            input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n",
        "            x.append(np.array(input_ids))\n",
        "            t.append(token_type_ids)\n",
        "        return np.array(x), np.array(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xf3FACEDaMU"
      },
      "source": [
        "bert_ckpt_dir=\"gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/\"\n",
        "bert_ckpt_file = bert_ckpt_dir + \"bert_model.ckpt\"\n",
        "bert_config_file = bert_ckpt_dir + \"bert_config.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5cdWD42DaO1",
        "outputId": "7063e91a-54a8-4859-bf0e-79f2470eee00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "%%time\n",
        "\n",
        "bert_model_dir=\"2018_10_18\"\n",
        "bert_model_name=\"uncased_L-12_H-768_A-12\"\n",
        "\n",
        "!mkdir -p .model .model/$bert_model_name\n",
        "\n",
        "for fname in [\"bert_config.json\", \"vocab.txt\", \"bert_model.ckpt.meta\", \"bert_model.ckpt.index\", \"bert_model.ckpt.data-00000-of-00001\"]:\n",
        "  cmd = f\"gsutil cp gs://bert_models/{bert_model_dir}/{bert_model_name}/{fname} .model/{bert_model_name}\"\n",
        "  !$cmd\n",
        "\n",
        "!ls -la .model .model/$bert_model_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_config.json...\n",
            "/ [0 files][    0.0 B/  313.0 B]                                                \r/ [1 files][  313.0 B/  313.0 B]                                                \r\n",
            "Operation completed over 1 objects/313.0 B.                                      \n",
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/vocab.txt...\n",
            "/ [1 files][226.1 KiB/226.1 KiB]                                                \n",
            "Operation completed over 1 objects/226.1 KiB.                                    \n",
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt.meta...\n",
            "/ [1 files][883.0 KiB/883.0 KiB]                                                \n",
            "Operation completed over 1 objects/883.0 KiB.                                    \n",
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt.index...\n",
            "/ [1 files][  8.3 KiB/  8.3 KiB]                                                \n",
            "Operation completed over 1 objects/8.3 KiB.                                      \n",
            "Copying gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001...\n",
            "- [1 files][420.0 MiB/420.0 MiB]                                                \n",
            "Operation completed over 1 objects/420.0 MiB.                                    \n",
            ".model:\n",
            "total 12\n",
            "drwxr-xr-x 3 root root 4096 Nov 10 17:38 .\n",
            "drwxr-xr-x 1 root root 4096 Nov 10 19:23 ..\n",
            "drwxr-xr-x 2 root root 4096 Nov 10 19:53 uncased_L-12_H-768_A-12\n",
            "\n",
            ".model/uncased_L-12_H-768_A-12:\n",
            "total 431244\n",
            "drwxr-xr-x 2 root root      4096 Nov 10 19:53 .\n",
            "drwxr-xr-x 3 root root      4096 Nov 10 17:38 ..\n",
            "-rw-r--r-- 1 root root       313 Nov 10 19:52 bert_config.json\n",
            "-rw-r--r-- 1 root root 440425712 Nov 10 19:53 bert_model.ckpt.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root      8528 Nov 10 19:52 bert_model.ckpt.index\n",
            "-rw-r--r-- 1 root root    904243 Nov 10 19:52 bert_model.ckpt.meta\n",
            "-rw-r--r-- 1 root root    231508 Nov 10 19:52 vocab.txt\n",
            "CPU times: user 56.6 ms, sys: 254 ms, total: 310 ms\n",
            "Wall time: 14.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSIuu6ifDaRR"
      },
      "source": [
        "bert_ckpt_dir    = os.path.join(\".model/\",bert_model_name)\n",
        "bert_ckpt_file   = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\n",
        "bert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVVTRJxFDaTz",
        "outputId": "65ef156e-9fb4-4ff6-b609-009e798a91d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\n",
        "data = HealthReviewData(tokenizer, \n",
        "                       sample_size=10*256*2,#5000, \n",
        "                       max_seq_len=128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos: 100%|██████████| 1137/1137 [07:20<00:00,  2.58it/s]\n",
            "neg: 100%|██████████| 61/61 [00:22<00:00,  2.68it/s]\n",
            "pos: 100%|██████████| 128/128 [00:48<00:00,  2.65it/s]\n",
            "neg: 100%|██████████| 8/8 [00:02<00:00,  2.70it/s]\n",
            "100%|██████████| 1.20k/1.20k [00:17<00:00, 68.8it/s]\n",
            "100%|██████████| 136/136 [00:02<00:00, 65.2it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "max seq_len 10337\n",
            "CPU times: user 22.3 s, sys: 800 ms, total: 23.1 s\n",
            "Wall time: 9min 9s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8XjRyboDaWW",
        "outputId": "83891199-89b9-46a9-a9e7-394cfdda5fea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"            train_x\", data.train_x.shape)\n",
        "print(\"train_x_token_types\", data.train_x_token_types.shape)\n",
        "print(\"            train_y\", data.train_y.shape)\n",
        "\n",
        "print(\"             test_x\", data.test_x.shape)\n",
        "\n",
        "print(\"        max_seq_len\", data.max_seq_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            train_x (1198, 128)\n",
            "train_x_token_types (1198, 128)\n",
            "            train_y (1198,)\n",
            "             test_x (136, 128)\n",
            "        max_seq_len 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QartoP2dDMY9"
      },
      "source": [
        "def flatten_layers(root_layer):\n",
        "    if isinstance(root_layer, keras.layers.Layer):\n",
        "        yield root_layer\n",
        "    for layer in root_layer._layers:\n",
        "        for sub_layer in flatten_layers(layer):\n",
        "            yield sub_layer\n",
        "\n",
        "\n",
        "def freeze_bert_layers(l_bert):\n",
        "    \"\"\"\n",
        "    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.\n",
        "    \"\"\"\n",
        "    for layer in flatten_layers(l_bert):\n",
        "        if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
        "            layer.trainable = True\n",
        "        elif len(layer._layers) == 0:\n",
        "            layer.trainable = False\n",
        "        l_bert.embeddings_layer.trainable = False\n",
        "\n",
        "\n",
        "def create_learning_rate_scheduler(max_learn_rate=5e-5,\n",
        "                                   end_learn_rate=1e-7,\n",
        "                                   warmup_epoch_count=10,\n",
        "                                   total_epoch_count=90):\n",
        "\n",
        "    def lr_scheduler(epoch):\n",
        "        if epoch < warmup_epoch_count:\n",
        "            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)\n",
        "        else:\n",
        "            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))\n",
        "        return float(res)\n",
        "    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\n",
        "\n",
        "    return learning_rate_scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJf9UHyiDMbh"
      },
      "source": [
        "def create_model(max_seq_len, adapter_size=64):\n",
        "  \"\"\"Creates a classification model.\"\"\"\n",
        "\n",
        "  #adapter_size = 64  # see - arXiv:1902.00751\n",
        "\n",
        "  # create the bert layer\n",
        "  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n",
        "      bc = StockBertConfig.from_json_string(reader.read())\n",
        "      bert_params = map_stock_config_to_params(bc)\n",
        "      bert_params.adapter_size = adapter_size\n",
        "      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
        "        \n",
        "  input_ids      = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"input_ids\")\n",
        "  # token_type_ids = keras.layers.Input(shape=(max_seq_len,), dtype='int32', name=\"token_type_ids\")\n",
        "  # output         = bert([input_ids, token_type_ids])\n",
        "  output         = bert(input_ids)\n",
        "\n",
        "  print(\"bert shape\", output.shape)\n",
        "  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(output)\n",
        "  cls_out = keras.layers.Dropout(0.5)(cls_out)\n",
        "  logits = keras.layers.Dense(units=768, activation=\"tanh\")(cls_out)\n",
        "  logits = keras.layers.Dropout(0.5)(logits)\n",
        "  logits = keras.layers.Dense(units=2, activation=\"softmax\")(logits)\n",
        "\n",
        "  # model = keras.Model(inputs=[input_ids, token_type_ids], outputs=logits)\n",
        "  # model.build(input_shape=[(None, max_seq_len), (None, max_seq_len)])\n",
        "  model = keras.Model(inputs=input_ids, outputs=logits)\n",
        "  model.build(input_shape=(None, max_seq_len))\n",
        "\n",
        "  # load the pre-trained model weights\n",
        "  load_stock_weights(bert, bert_ckpt_file)\n",
        "\n",
        "  # freeze weights if adapter-BERT is used\n",
        "  if adapter_size is not None:\n",
        "      freeze_bert_layers(bert)\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")])\n",
        "\n",
        "  model.summary()\n",
        "        \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bKsRh0BDMdu",
        "outputId": "234e2805-c666-410f-87da-142a62e68283",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "adapter_size = None # use None to fine-tune all of BERT\n",
        "model = create_model(data.max_seq_len, adapter_size=adapter_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert shape (None, 128, 768)\n",
            "Done loading 196 BERT weights from: .model/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7eff0b3872e8> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
            "Unused weights from checkpoint: \n",
            "\tbert/embeddings/token_type_embeddings\n",
            "\tbert/pooler/dense/bias\n",
            "\tbert/pooler/dense/kernel\n",
            "\tcls/predictions/output_bias\n",
            "\tcls/predictions/transform/LayerNorm/beta\n",
            "\tcls/predictions/transform/LayerNorm/gamma\n",
            "\tcls/predictions/transform/dense/bias\n",
            "\tcls/predictions/transform/dense/kernel\n",
            "\tcls/seq_relationship/output_bias\n",
            "\tcls/seq_relationship/output_weights\n",
            "Model: \"functional_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_ids (InputLayer)       [(None, 128)]             0         \n",
            "_________________________________________________________________\n",
            "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
            "_________________________________________________________________\n",
            "lambda_6 (Lambda)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 768)               590592    \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 2)                 1538      \n",
            "=================================================================\n",
            "Total params: 109,482,242\n",
            "Trainable params: 109,482,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39iDr9jaDMfz",
        "outputId": "d5f3079d-6f30-447a-d51e-46a440d3fb1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "log_dir = \".log/health_news_reviews/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\")\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
        "\n",
        "total_epoch_count = 50\n",
        "# model.fit(x=(data.train_x, data.train_x_token_types), y=data.train_y,\n",
        "model.fit(x=data.train_x, y=data.train_y,\n",
        "          validation_split=0.1,\n",
        "          batch_size=48,\n",
        "          shuffle=True,\n",
        "          epochs=total_epoch_count,\n",
        "          callbacks=[create_learning_rate_scheduler(max_learn_rate=1e-5,\n",
        "                                                    end_learn_rate=1e-7,\n",
        "                                                    warmup_epoch_count=20,\n",
        "                                                    total_epoch_count=total_epoch_count),\n",
        "                     keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),\n",
        "                     tensorboard_callback])\n",
        "\n",
        "model.save_weights('./health_news_reviews10.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 5.000000000000001e-07.\n",
            "Epoch 1/50\n",
            "23/23 [==============================] - 37s 2s/step - loss: 0.7856 - acc: 0.3840 - val_loss: 0.6776 - val_acc: 0.5417\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 1.0000000000000002e-06.\n",
            "Epoch 2/50\n",
            "23/23 [==============================] - 34s 1s/step - loss: 0.5609 - acc: 0.7801 - val_loss: 0.4046 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 1.5000000000000002e-06.\n",
            "Epoch 3/50\n",
            "23/23 [==============================] - 34s 1s/step - loss: 0.4173 - acc: 0.9369 - val_loss: 0.3417 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 2.0000000000000003e-06.\n",
            "Epoch 4/50\n",
            "23/23 [==============================] - 35s 2s/step - loss: 0.3815 - acc: 0.9443 - val_loss: 0.3332 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 2.5000000000000006e-06.\n",
            "Epoch 5/50\n",
            "23/23 [==============================] - 35s 2s/step - loss: 0.3735 - acc: 0.9453 - val_loss: 0.3313 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 3.0000000000000005e-06.\n",
            "Epoch 6/50\n",
            "23/23 [==============================] - 35s 2s/step - loss: 0.3707 - acc: 0.9453 - val_loss: 0.3307 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 3.5000000000000004e-06.\n",
            "Epoch 7/50\n",
            "23/23 [==============================] - 35s 2s/step - loss: 0.3698 - acc: 0.9453 - val_loss: 0.3304 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 4.000000000000001e-06.\n",
            "Epoch 8/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3694 - acc: 0.9453 - val_loss: 0.3302 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 4.500000000000001e-06.\n",
            "Epoch 9/50\n",
            "23/23 [==============================] - 35s 2s/step - loss: 0.3690 - acc: 0.9453 - val_loss: 0.3301 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 5.000000000000001e-06.\n",
            "Epoch 10/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3687 - acc: 0.9453 - val_loss: 0.3301 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 5.500000000000001e-06.\n",
            "Epoch 11/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3686 - acc: 0.9453 - val_loss: 0.3300 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 6.000000000000001e-06.\n",
            "Epoch 12/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3684 - acc: 0.9453 - val_loss: 0.3300 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 6.500000000000001e-06.\n",
            "Epoch 13/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3683 - acc: 0.9453 - val_loss: 0.3300 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 7.000000000000001e-06.\n",
            "Epoch 14/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3682 - acc: 0.9453 - val_loss: 0.3300 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 7.500000000000001e-06.\n",
            "Epoch 15/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3682 - acc: 0.9453 - val_loss: 0.3300 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 8.000000000000001e-06.\n",
            "Epoch 16/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3682 - acc: 0.9453 - val_loss: 0.3300 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 8.500000000000002e-06.\n",
            "Epoch 17/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 9.000000000000002e-06.\n",
            "Epoch 18/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 9.500000000000002e-06.\n",
            "Epoch 19/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 1.0000000000000003e-05.\n",
            "Epoch 20/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 8.619535664753031e-06.\n",
            "Epoch 21/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 7.429639507594948e-06.\n",
            "Epoch 22/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 6.404004271197281e-06.\n",
            "Epoch 23/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 5.51995432128157e-06.\n",
            "Epoch 24/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 4.757944314009411e-06.\n",
            "Epoch 25/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 4.101127070551301e-06.\n",
            "Epoch 26/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 3.5349811050301065e-06.\n",
            "Epoch 27/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 3.046989570903508e-06.\n",
            "Epoch 28/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 2.626363527653332e-06.\n",
            "Epoch 29/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 2.263803409521448e-06.\n",
            "Epoch 30/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3680 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 1.9512934226359633e-06.\n",
            "Epoch 31/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 1.6819243248808695e-06.\n",
            "Epoch 32/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 1.4497406703726316e-06.\n",
            "Epoch 33/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3680 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 1.2496091412919872e-06.\n",
            "Epoch 34/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3680 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 1.0771050560367686e-06.\n",
            "Epoch 35/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3680 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 9.284145445194742e-07.\n",
            "Epoch 36/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 8.002502278161051e-07.\n",
            "Epoch 37/50\n",
            "23/23 [==============================] - 37s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 6.897785379387655e-07.\n",
            "Epoch 38/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 5.945570708544392e-07.\n",
            "Epoch 39/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3680 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 5.124805876960932e-07.\n",
            "Epoch 40/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 4.417344703140068e-07.\n",
            "Epoch 41/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to 3.807546021222372e-07.\n",
            "Epoch 42/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3680 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to 3.281927872511473e-07.\n",
            "Epoch 43/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3680 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to 2.8288694346259687e-07.\n",
            "Epoch 44/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3680 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to 2.4383540982688284e-07.\n",
            "Epoch 45/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3680 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to 2.1017480113324875e-07.\n",
            "Epoch 46/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3680 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to 1.8116091942004138e-07.\n",
            "Epoch 47/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to 1.5615230060004974e-07.\n",
            "Epoch 48/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to 1.3459603241553634e-07.\n",
            "Epoch 49/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3680 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to 1.1601553017399705e-07.\n",
            "Epoch 50/50\n",
            "23/23 [==============================] - 36s 2s/step - loss: 0.3681 - acc: 0.9453 - val_loss: 0.3299 - val_acc: 0.9833\n",
            "CPU times: user 13min 28s, sys: 2min 58s, total: 16min 26s\n",
            "Wall time: 31min 33s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjiCUgPWEn21",
        "outputId": "3497c0f4-d994-45cd-ab4f-e845c7e5968d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time \n",
        "\n",
        "model = create_model(data.max_seq_len, adapter_size=None)\n",
        "model.load_weights(\"health_news_reviews10.h5\")\n",
        "\n",
        "_, train_acc = model.evaluate(data.train_x, data.train_y)\n",
        "_, test_acc = model.evaluate(data.test_x, data.test_y)\n",
        "\n",
        "print(\"train acc\", train_acc)\n",
        "print(\" test acc\", test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert shape (None, 128, 768)\n",
            "Done loading 196 BERT weights from: .model/uncased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7f02aa3072b0> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
            "Unused weights from checkpoint: \n",
            "\tbert/embeddings/token_type_embeddings\n",
            "\tbert/pooler/dense/bias\n",
            "\tbert/pooler/dense/kernel\n",
            "\tcls/predictions/output_bias\n",
            "\tcls/predictions/transform/LayerNorm/beta\n",
            "\tcls/predictions/transform/LayerNorm/gamma\n",
            "\tcls/predictions/transform/dense/bias\n",
            "\tcls/predictions/transform/dense/kernel\n",
            "\tcls/seq_relationship/output_bias\n",
            "\tcls/seq_relationship/output_weights\n",
            "Model: \"functional_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_ids (InputLayer)       [(None, 128)]             0         \n",
            "_________________________________________________________________\n",
            "bert (BertModelLayer)        (None, 128, 768)          108890112 \n",
            "_________________________________________________________________\n",
            "lambda_7 (Lambda)            (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 768)               590592    \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 768)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 2)                 1538      \n",
            "=================================================================\n",
            "Total params: 109,482,242\n",
            "Trainable params: 109,482,242\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "38/38 [==============================] - 12s 311ms/step - loss: 0.3642 - acc: 0.9491\n",
            "5/5 [==============================] - 1s 218ms/step - loss: 0.3721 - acc: 0.9412\n",
            "train acc 0.9490817785263062\n",
            " test acc 0.9411764740943909\n",
            "CPU times: user 9.46 s, sys: 1.17 s, total: 10.6 s\n",
            "Wall time: 20.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEM9rJ63mesY"
      },
      "source": [
        "sentences = data.test_x\n",
        "test_labels = data.test_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj-BgQOmjv-T"
      },
      "source": [
        "predictions = model.predict(sentences).argmax(axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiNpWoanEn50"
      },
      "source": [
        "import collections\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "def get_evaluation_metrics(reference, result):\n",
        "\n",
        "    evaluation_metrics = collections.namedtuple('evaluation_metrics', ['accuracy', 'avg_recall','avg_precision','avg_F1']) \n",
        "    accuracy = accuracy_score(reference,result)  \n",
        "    avg_recall = recall_score(reference,result,average='weighted')    \n",
        "    avg_precision = precision_score(reference,result,average='weighted')    \n",
        "    avg_F1 = f1_score(reference,result,average='weighted')  \n",
        "   \n",
        "    evm = evaluation_metrics(accuracy,avg_recall,avg_precision,avg_F1)\n",
        "    return evm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c76YUMVEn8e",
        "outputId": "7f9df05a-9020-4f4f-d09d-5c2a8715d716",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evaluations =  get_evaluation_metrics(list(test_labels), predictions)\n",
        "print ('          {}        {}        {}  '.format(round(evaluations[1]*100,2), round(evaluations[2]*100,2),round(evaluations[3]*100,2)))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          94.12        88.58        91.27  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlq2eMK7En-R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTSCtfIwEoDG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swEbc6_8EoGd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTvMU9_vEoBD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htH-QeNp-3WM"
      },
      "source": [
        ""
      ]
    }
  ]
}